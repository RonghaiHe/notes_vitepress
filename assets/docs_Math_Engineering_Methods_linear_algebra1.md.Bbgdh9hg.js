import{_ as n,c as s,o,ah as a,j as t,a as e}from"./chunks/framework.BA564An1.js";const p=JSON.parse('{"title":"Introduction of Linear algebra","description":"","frontmatter":{"comments":true},"headers":[],"relativePath":"docs/Math/Engineering_Methods/linear_algebra1.md","filePath":"docs/Math/Engineering_Methods/linear_algebra1.md","lastUpdated":1757673252000}'),r={name:"docs/Math/Engineering_Methods/linear_algebra1.md"};function $(l,i,d,c,m,h){return o(),s("div",null,i[0]||(i[0]=[a('<h1 id="introduction-of-linear-algebra" tabindex="-1">Introduction of Linear algebra <a class="header-anchor" href="#introduction-of-linear-algebra" aria-label="Permalink to “Introduction of Linear algebra”">​</a></h1><h2 id="sets" tabindex="-1">sets <a class="header-anchor" href="#sets" aria-label="Permalink to “sets”">​</a></h2><p>!!!note &quot;Definition&quot; <strong>sets</strong>: collection of objects, called <strong>elements</strong> of the set<br> - $x\\in A; x\\notin A$ - list them using ${}$ - no order: ${1,2}={2,1}$<br></p><p><strong>subsets</strong>:</p><ul><li>$B\\subseteq A$</li><li>proper set: $B\\subseteq A, B\\neq A$</li><li>empty set: $\\phi$</li></ul><p><strong>calculation</strong></p><ul><li><p>union: $A\\cup B$</p></li><li><p>intersection: $A \\cap B$</p><ul><li>disjoint sets: $A\\cap B=\\phi$</li></ul></li><li><p>finite sets: ${1,2}$</p></li><li><p>infinite sets: $\\mathbb{R}$</p></li></ul><p>Number systems:</p><ul><li>$\\mathbb{N}(\\mathbb{N}_0: (\\text{ include }0); \\mathbb{N}_1: (\\text{ not include }0))$</li><li>$\\mathbb{Z}$</li><li>Rational number: $\\mathbb{Q}$</li><li>Real number: $\\mathbb{R}$</li><li>Complex number: $\\mathbb{C}$</li></ul><p>Cardinal number: the number if elements of a set</p><p>For infinite sets:</p><ul><li>Countable sets: aleph zero: $\\mathbb{N},\\mathbb{Z},\\mathbb{Q}$</li><li>Uncountable sets: beth one</li></ul><h2 id="operations" tabindex="-1">operations <a class="header-anchor" href="#operations" aria-label="Permalink to “operations”">​</a></h2><p>operation: from a set to itself</p><p>Unary:</p><ul><li>negation: $-$</li><li>factorial: $!$</li><li>trigonometric: $\\sin,\\cos$</li></ul><p>Binary:</p><ul><li>$+-x/$</li></ul><p>tenary:...</p><h3 id="arithmetic" tabindex="-1">Arithmetic <a class="header-anchor" href="#arithmetic" aria-label="Permalink to “Arithmetic”">​</a></h3><ul><li><p>$+,\\times$</p><table tabindex="0"><thead><tr><th>Set</th><th>$\\mathbb{N}_0$</th><th>$\\mathbb{N}_1$</th></tr></thead><tbody><tr><td>Elements</td><td>$0,1,2,3\\in\\mathbb{N}_0$</td><td>$1,2,3,4\\in\\mathbb{N}_1$</td></tr><tr><td>Operation</td><td>$+$</td><td>$\\times$</td></tr><tr><td>Closure</td><td>$1+2=3\\in\\mathbb{N}_0$</td><td>$2\\times 3=6\\in\\mathbb{N}_1$</td></tr><tr><td>Associative</td><td>$(1+2)+3=1+(2+3)$</td><td>$(2\\times 3)\\times 4=2\\times(3\\times 4)$</td></tr><tr><td>Commutative</td><td>$1+2=2+1$</td><td>$2\\times 3=3\\times 2$</td></tr><tr><td>Distributive</td><td>-</td><td>-</td></tr></tbody></table></li><li><p>$-,/$</p><table tabindex="0"><thead><tr><th>Set</th><th>$\\mathbb{Z}$</th><th>$\\mathbb{Q}$</th></tr></thead><tbody><tr><td>Elements</td><td>$x,y,\\cdots\\in\\mathbb{Z}$</td><td>$x,y\\cdots\\in\\mathbb{Q}$</td></tr><tr><td>Operation</td><td>$+$</td><td>$x$</td></tr><tr><td>Identity</td><td>$0$</td><td>$1$</td></tr><tr><td>inverses</td><td>$-x$</td><td>$\\frac{1}{x}$ or $x^{-1}$</td></tr></tbody></table></li></ul><p>More generally:</p><table tabindex="0"><thead><tr><th>Set</th><th>$ S$</th><th>$\\mathbb{Z}$</th><th>$\\mathbb{Q}$</th></tr></thead><tbody><tr><td>Elements</td><td>$x,y,z,\\cdots\\in S$</td><td>$x,y,z,\\cdots\\in\\mathbb{Z}$</td><td>$x,y,z,\\cdots\\in\\mathbb{Q}$</td></tr><tr><td>Operation</td><td>$*$</td><td>$+$</td><td>$\\times$</td></tr><tr><td>Closure</td><td>$x*y\\in S$</td><td>$x+y\\in\\mathbb{Z}$</td><td>$x\\times y\\in\\mathbb{Q}$</td></tr><tr><td>Associative</td><td>$(x<em>y)<em>z=x</em>(y</em>z)$</td><td>$(x+y)+z=x+(y+z)$</td><td>$(x\\times y)\\times z=x\\times(y\\times z)$</td></tr><tr><td>Commutative</td><td>$x<em>y=y</em>x$</td><td>$x+y=y+x$</td><td>$x\\times y=y\\times x$</td></tr><tr><td>Distributive</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Identity</td><td>$\\exists e\\in S: x<em>e=e</em>x=x$</td><td>$0$</td><td>$1$</td></tr><tr><td>inverses</td><td>$\\forall x,\\exists x^{-1}\\in S: x*x^{-1}=x^{-1}*x=e$</td><td>$-x$</td><td>$\\frac{1}{x}$ or $x^{-1}$</td></tr></tbody></table><h2 id="group" tabindex="-1">group <a class="header-anchor" href="#group" aria-label="Permalink to “group”">​</a></h2><p>Given:</p><ul><li>Set of elements $G$</li><li>Operation: $*$</li></ul><table tabindex="0"><thead><tr><th>Set</th><th>$G$</th></tr></thead><tbody><tr><td>Elements</td><td>$x,y,z,\\cdots\\in G$</td></tr><tr><td>Operation</td><td>$*$</td></tr><tr><td>Closure</td><td>$x*y\\in G$</td></tr><tr><td>Associative</td><td>$(x<em>y)<em>z=x</em>(y</em>z)$</td></tr><tr><td>Commutative</td><td>$x<em>y=y</em>x$? (Not requestes)</td></tr><tr><td>Distributive</td><td>-</td></tr><tr><td>Identity</td><td>$\\exists e\\in G: x<em>e=e</em>x=x$</td></tr><tr><td>inverses</td><td>$\\forall x,\\exists x^{-1}\\in G: x*x^{-1}=x^{-1}*x=e$</td></tr></tbody></table><ul><li>If communtative: commutative group / abelian group</li><li>If not commutative: non-commutative group / non-abelian group</li></ul><p>Example of group:</p><ul><li>commutative: <ul><li>$\\mathbb{Z}$ with $+$</li><li>$\\mathbb{Z}$ commutes under $+$</li></ul></li><li>non-commutative: <ul><li>Dihedral group TBC</li></ul></li></ul><h2 id="ring" tabindex="-1">Ring <a class="header-anchor" href="#ring" aria-label="Permalink to “Ring”">​</a></h2><p>Given:</p><ul><li>Set of elements $R$</li><li>Operation: $+, \\times$</li></ul><table tabindex="0"><thead><tr><th>Set</th><th>$R$</th><th></th></tr></thead><tbody><tr><td>Elements</td><td>$a,b,c,\\cdots\\in R$</td><td></td></tr><tr><td>Operation</td><td>Addition $+$</td><td>Multiplication $\\times$</td></tr><tr><td>Closure</td><td>$a+b\\in R$</td><td>$a\\times b\\in R$</td></tr><tr><td>Associative</td><td>$(a+b)+c=a+(b+c)$</td><td>$(a\\times b)\\times c=a\\times(b\\times c)$</td></tr><tr><td>Commutative</td><td>$a+b=b+a$</td><td>$a\\times b=b\\times a$? (Not required)</td></tr><tr><td>Distributive</td><td>$a\\times(b+c)=a\\times b + a\\times c; \\ (b+c)\\times a=b\\times a+c\\times a$</td><td></td></tr><tr><td>Identity</td><td>$\\exists 0\\in R: a+0=0+a=a$</td><td>$\\exists 1\\in R: 1\\times a = a\\times 1 = a$</td></tr><tr><td>Inverses</td><td>$\\exists (-a)\\in R: a+(-a)=0$</td><td>$\\exists a^{-1}\\in R$? (Not required)</td></tr></tbody></table><ul><li>distributive: left distributive v.s. right distributive</li><li>If communitative, just check one distributivity</li></ul><p>Example of ring:</p><ul><li>commutative: <ul><li>$\\mathbb{Z}$ with $+,\\times$</li></ul></li><li>non-commutative: <ul><li>$M_2(R)$ with matrix addition and matrix multiplication</li></ul></li></ul><h2 id="field" tabindex="-1">Field <a class="header-anchor" href="#field" aria-label="Permalink to “Field”">​</a></h2><p>Given:</p><ul><li>Set of elements $F$</li><li>Operation: $+, \\times$</li></ul><table tabindex="0"><thead><tr><th>Set</th><th>$F$</th></tr></thead><tbody><tr><td>Elements</td><td>$a,b,c,\\cdots\\in F$</td></tr><tr><td>Operation</td><td>Addition $+$</td></tr><tr><td>Closure</td><td>$a+b\\in F$</td></tr><tr><td>Associative</td><td>$(a+b)+c=a+(b+c)$</td></tr><tr><td>Commutative</td><td>$a+b=b+a$</td></tr><tr><td>Distributive</td><td>$a\\times(b+c)=a\\times b + a\\times c$</td></tr><tr><td>Identity</td><td>$\\exists 0\\in F: a+0=0+a=a$</td></tr><tr><td>inverses</td><td>$\\exists (-a)\\in F: a+(-a)=0$</td></tr></tbody></table><p>Example of field:</p><ul><li>$\\mathbb{Q}$ with $+,\\times$</li><li>$\\mathbb{R}$ with $+,\\times$</li><li>$\\mathbb{C}$ with $+,\\times$</li><li>$\\operatorname{GF}(2) / Z_2$, a finite field with two elements with XOR and AND.</li></ul><p>Modulo Arithmetic:</p><ul><li>$17 \\pmod 5 = 2$</li><li>$17 \\equiv 2 (\\pmod 5)$</li></ul><p><strong>Theorem C.1</strong> (Cancellation Laws):</p><p>$\\forall a,b,c$ in a field, the following statements are true:</p><ul><li>If $a+b=c+b$, then $a=c$</li><li>If $a\\cdot b=c\\cdot b,b\\neq 0$, then $a=c$</li></ul><p><strong>Corollary</strong>: the identity elements and the inverse elements are <strong>unique</strong></p><p><strong>Theorem C.2</strong>:</p><p>$\\forall a,b$ in a field, the following statements are true:</p><ul><li>$a\\cdot 0=0$</li><li>$(-a)\\cdot b=a\\cdot(-b)=-(a\\cdot b)$</li><li>$(-a)\\cdot(-b)=a\\cdot b$</li></ul><h2 id="vector-space" tabindex="-1">vector space <a class="header-anchor" href="#vector-space" aria-label="Permalink to “vector space”">​</a></h2><p>Given:</p><ul><li>Set of elements $V, F$</li><li>Operation: $+, \\times$</li><li>Commutative group $V$ under $+$, with a Field $F$</li></ul><table tabindex="0"><thead><tr><th>Set</th><th>$V$</th><th>$V$ and $F$</th><th>$F$</th><th>$F$</th></tr></thead><tbody><tr><td>Elements</td><td>$\\vec{u},\\vec{v},\\vec{w}\\in V$</td><td></td><td>$a,b,c\\in F$</td><td></td></tr><tr><td>Operation</td><td>Addition $+$</td><td>Scalar multiplication $\\times$</td><td>Addition $+$</td><td>Multiplication $\\times$</td></tr><tr><td>Closure</td><td>$\\vec{u}+\\vec{v}\\in V$</td><td>$a\\times \\vec{u}\\in V$</td><td>$a+b\\in F$</td><td>$a\\times b\\in F$</td></tr><tr><td>Associative</td><td>$(\\vec{u}+\\vec{v})+\\vec{w} = \\vec{u}+(\\vec{v}+\\vec{w})$</td><td>$(a\\times b)\\times \\vec{u}=a\\times(b\\times \\vec{u})$</td><td>$(a+b)+c=a+(b+c)$</td><td>$(a\\times b)\\times c=a\\times(b\\times c)$</td></tr><tr><td>Commutative</td><td>$\\vec{u}+\\vec{v}=\\vec{v}+\\vec{u}$</td><td>\\color{gray}{$a\\times \\vec{u}=\\vec{u}\\times a$}</td><td>$a+b=b+a$</td><td>$a\\times b=b\\times a$</td></tr><tr><td>Distributive</td><td>-</td><td>$a\\times(\\vec{u}+\\vec{v})=a\\times\\vec{u}+a\\times\\vec{v}; (a+b)\\times\\vec{u}=a\\times\\vec{u}+b\\times\\vec{u}$</td><td>$a\\times(b+c)=a\\times b + a\\times c$</td><td></td></tr><tr><td>Identity</td><td>$\\exists \\vec{0}\\in V: \\vec{u}+\\vec{0}=\\vec{0}+\\vec{u}=\\vec{u}$</td><td>$1\\times \\vec{u}=\\vec{u}$</td><td>$\\exists 0\\in F: a+0=0+a=a$</td><td>$\\exists 1\\in F: 1\\times a = a\\times 1 = a$</td></tr><tr><td>inverses</td><td>$\\exists (-\\vec{u})\\in V: \\vec{u}+(-\\vec{u})=\\vec{0}$</td><td>\\color{gray}{$0\\times u=\\vec{0}; (-1)\\times \\vec{u}=-\\vec{u}$}</td><td>$\\exists (-a)\\in F: a+(-a)=0$</td><td>$\\exists a^{-1}\\in F: a\\times a^{-1}=1$</td></tr></tbody></table><p>Module definition: similar to vector space, but:</p><ul><li>commutative of scalar multiplication not required</li><li>commutative of multiplication for number part, not required</li></ul><p>Definition:</p><ul><li>sum: $x+y$</li><li>product: $ax$</li><li>scalars: elements of $F$</li><li>vectors: elements of vector space $\\mathsf{V}$</li><li>n-tuple: $n$ elements of a field in this form: $(a_1,\\cdots,a_n)$ <ul><li>entries / components: $a_1,\\cdots,a_n$</li><li>2 n-tuples are equal if $a_i=b_i, \\forall i=01,2,\\cdots,n$</li><li>$\\mathsf{F}^n$: set of all n-tuples with entries from a field $F$</li><li>vectors in $\\mathsf{F}^n$: column vectors</li></ul></li></ul><h2 id="addition-and-scalar-multiplication" tabindex="-1">addition and scalar multiplication <a class="header-anchor" href="#addition-and-scalar-multiplication" aria-label="Permalink to “addition and scalar multiplication”">​</a></h2><h3 id="matrix" tabindex="-1">matrix <a class="header-anchor" href="#matrix" aria-label="Permalink to “matrix”">​</a></h3><p>Definitions:</p><ul><li>diagonal entries: $a_{ij}$ with $i=j$</li><li>i-th row: $a_{i1},a_{i2},\\cdots,a_{in}$</li><li>j-th column: $a_{1j},a_{2j},\\cdots,a_{mj}$</li><li>zero matrix: all zero</li><li>square: the number of rows and columns are equal</li><li>equal: $A_{ij}=B_{ij}, \\forall 1\\le i\\le m, 1\\le j\\le n$</li><li>set of all $m\\times n$ matrices with entries from a field $F$ is a vector space: $\\mathsf{M}_{m\\times n}(F)$</li></ul>',64),t("p",null,[e("matrix addition: $(A+B)"),t("em",{ij:""},"{ij}=A"),e("+B_{ij}$ scalar multiplication: $(cA)"),t("em",{ij:""},"{ij}=cA"),e("$")],-1),a('<h3 id="function" tabindex="-1">function <a class="header-anchor" href="#function" aria-label="Permalink to “function”">​</a></h3><p>Let $S$ be any nonempty set and $F$ be any field, and let $\\mathcal{F}(S, F)$ denote the set of all functions from $S$ to $F$. Two functions $f$ and $g$ in $\\mathcal{F}(S, F)$ are called <strong>equal</strong> if $f(s)=g(s)$ for each $s\\in S$. The set $\\mathcal{F}(S, F)$ is a vector space with the operations of <strong>addition and scalar multiplication</strong> defined for $f, g \\in \\mathcal{F}(S, F)$ and $c\\in F$ by</p><p>$$(f+g)(s)=f(s)+g(s) \\text{ and } (cf)(s)=c[f(s)]$$</p><p>for each $s \\in S$. Note that these are the familiar operations of <strong>addition and scalar multiplication</strong> for functions used in algebra and calculus.</p><h3 id="polynominal" tabindex="-1">polynominal <a class="header-anchor" href="#polynominal" aria-label="Permalink to “polynominal”">​</a></h3><p>$$f(x)=a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x_1+a_0$$</p><ul><li>coefficient: $a_i, i=0,1,\\cdots,n$</li><li>zero polynominal: $a_i=0, i=0,1,\\cdots,n$</li><li>degree: <ul><li>$-1$ for zero polynominal</li><li>largest exponent of $x$</li></ul></li><li>equal if equal degree and $a_i=b_i, i=0,1,\\cdots,n$</li></ul><p>When $F$ is a field containing infinitely many scalars, we usually regard a polynomial with coefficients from $F$ as a function from $F$ into $F$</p><p>$$ \\begin{aligned} f(x) &amp;= a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x_1+a_0 \\ g(x) &amp;= b_nx^n+b_{n-1}x^{n-1}+\\cdots+b_1x_1+b_0 \\end{aligned} $$</p><p>addition and scalar multiplication: $$ \\begin{cases} f(x)+g(x)=(a_n+b_n)x^n + (a_{n-1}+b_{n-1})x^{n-1}+\\cdots+(a_0+b_0)\\ cf(x)= ca_nx^n+ca_{n-1}x^{n-1}+\\cdots+ca_1x_1+ca_0 \\end{cases} $$</p><p>set of all polynominal: $\\mathsf{P}(F)$</p><p><strong>Theorem 1.1</strong>: Cancellation Law for Vector Addition</p><p>If $x,y,z$ are vectors in a vector space, s.t. $x+z=y+z$, then $x=y$</p><p><strong>Corolloary 1</strong>: The vector $\\mathbf{0}$ is unique (zero vector)</p><p><strong>Corolloary 2</strong>: The inverse element of vector is unique (additive inverse)</p><p><strong>Theorem 1.2</strong>: In any vector space $\\mathsf{V}$:</p><ul><li>$0\\boldsymbol{x}=\\mathbf{0}, \\forall \\boldsymbol{x}\\in\\mathsf{V}$</li><li>$(-a)x=-(ax)=a(-x), \\forall a\\in F, x\\in \\mathsf{V}$</li><li>$a\\mathbf{0}=\\mathbf{0}, \\forall a\\in F$</li></ul><h2 id="subspace" tabindex="-1">subspace <a class="header-anchor" href="#subspace" aria-label="Permalink to “subspace”">​</a></h2><p>A subset $\\mathsf{W}$ of a vector space $\\mathsf{V}$ over a field $F$ is called a <strong>subspace</strong> of $\\mathsf{V}$ if $\\mathsf{W}$ is a <strong>vector space</strong> over $F$ with the <strong>operations of addition and scalar multiplication</strong> defined on $\\mathsf{V}$.</p><p>In any vector space $\\mathsf{V}$, note that $\\mathsf{V}$ and ${0}$ are subspaces. The latter is called the zero subspace of $\\mathsf{V}$.</p><p><strong>Theorem 1.3</strong>(subspace):Let $\\mathsf{V}$ be a vector space and $\\mathsf{W}$ a subset of $\\mathsf{V}$. Then $\\mathsf{W}$ is a subspace of $\\mathsf{V}$ iff the following three conditions hold for the operations defined in $\\mathsf{V}$.</p><ul><li>$0 \\in \\mathsf{W}$.</li><li>$x+y \\in \\mathsf{W}$ whenever $x \\in \\mathsf{W}$ and $y \\in \\mathsf{W}$.</li><li>$cx \\in \\mathsf{W}$ whenever $c\\in F$ and $x \\in \\mathsf{W}$.</li></ul><h3 id="examples" tabindex="-1">Examples <a class="header-anchor" href="#examples" aria-label="Permalink to “Examples”">​</a></h3><h4 id="matrix-1" tabindex="-1">matrix <a class="header-anchor" href="#matrix-1" aria-label="Permalink to “matrix”">​</a></h4>',24),t("p",null,[e("The "),t("strong",null,"transpose"),e(" $A^t$ of an $m \\times n$ matrix $A$ is the $n \\times m$ matrix obtained from $A$ by interchanging the rows with the columns; that is, $(A^t)"),t("em",{ji:""},"{ij} = A"),e("$.")],-1),t("p",null,[e("A "),t("strong",null,"symmetric"),e(" matrix is a matrix $A$ such that $A^t = A$.")],-1),t("ul",null,[t("li",null,[e("Clearly, a symmetric matrix must be "),t("strong",null,"square"),e(". The set $W$ of all symmetric matrices in $\\mathsf{M}"),t("em",{nxn:""},"{nxn}(F)$ is a subspace of $\\mathsf{M}"),e("(F)$ since the conditions of Theorem 1.3 hold")])],-1),a('<p>A diagonal matrix is a $n\\times n$ matrix if $M_{ij}=0$ whenever $i\\neq j$</p><ul><li>the set of diagonal matrices is a subspace of $\\mathsf{M}_{nxn}(F)$</li></ul><h4 id="polynominal-1" tabindex="-1">polynominal <a class="header-anchor" href="#polynominal-1" aria-label="Permalink to “polynominal”">​</a></h4><p>Let $n$ be nonnegative integer, $\\mathsf{P}_n(F)$ consists of all polynominals in $\\mathsf{P}(F)$ having degree less than or equal to $n$.</p><ul><li>$\\mathsf{P}_n(F)$ is a subspace of $\\mathsf{P}(F)$</li></ul><h3 id="theorem-and-definition" tabindex="-1">theorem and definition <a class="header-anchor" href="#theorem-and-definition" aria-label="Permalink to “theorem and definition”">​</a></h3><p><strong>Theorem1.4</strong>: Any intersecton of subspaces of a vector space $\\mathsf{V}$ is a subspace of $\\mathsf{V}$</p><p><strong>sum</strong> of nonempty subsets $S_1$ and $S_2$ of a vector space $\\mathsf{V}$ :</p><ul><li>$S_1+S_2:={x+y: x\\in S_1, y\\in S_2}$</li></ul><p><strong>direct sum</strong>: $\\mathsf{W}_1\\oplus\\mathsf{W}_2$</p><ul><li>$\\mathsf{W}_1,\\mathsf{W}_2$ are subspaces of $\\mathsf{V}$</li><li>$\\mathsf{W}_1\\cap\\mathsf{W}_2={0}, , \\mathsf{W}_1\\cup\\mathsf{W}_2=\\mathsf{V}$</li></ul><h2 id="_1-4-linear-combination-and-systems-of-linear-equations" tabindex="-1">1.4 linear combination and systems of linear equations <a class="header-anchor" href="#_1-4-linear-combination-and-systems-of-linear-equations" aria-label="Permalink to “1.4 linear combination and systems of linear equations”">​</a></h2><p><strong>Definition</strong>: Let $\\mathsf{V}$ be a vector space and $S$ a nonempty subset of $\\mathsf{V}$. A vector $\\vec{v} \\in \\mathsf{V}$ is called a <strong>linear combination</strong> of vectors of $S$ if there exists a finite number of vectors $\\vec{u}_1, \\vec{u}_2,.., \\vec{u}_n$ in $S$ and scalars $a_1, a_2,.., a_n$ in $F$ such that $\\vec{v}= a_1\\vec{u}_1+a_2\\vec{u}_2+...+ a_n\\vec{u}_n$. In this case we also say that $\\vec{v}$ is a linear combination of $\\vec{u}_1, \\vec{u}_2, ·.., \\vec{u}_n$ and call $a_1, a_2,·..$, An the <strong>coefficients</strong> of the linear combination.</p><p>Observe that in any vector space $\\mathsf{V}$, $O\\vec{v} = \\vec{0}$ for each $\\vec{v}\\in \\mathsf{V}$. Thus the zero vector is a linear combination of any nonempty subset of $\\mathsf{V}$.</p><p><strong>Definition</strong>. Let $s$ be a nonempty subset of a vector space $\\mathsf{V}$. The span of $S$, denoted $\\operatorname{span}(S)$, is the set consisting of <strong>all linear combinations of the vectors in $S$</strong>. For convenience, we define $\\operatorname{span}() = {0}$.</p><p>In $\\mathbb{R}^3$, for instance, the span of the set ${(1,0,0), (0,1,0)}$ consists of al vectors in $R3$ that have the form $a(1,0,0) + b(0,1,0) = (a,b,0)$ for some scalars $a$ and $b$. Thus the span of ${(1,0,0), (0,1, 0)}$ contains all the points in the xy-plane. In this case, the span of the set is a subspace of $\\mathbb{R}^3$. This fact is true in general.</p><p><strong>Theorem 1.5</strong>(span and subspace):</p><ul><li>The span of any subset $S$ of a vector space $\\mathsf{V}$ is a subspace of $\\mathsf{V}$.</li><li>Any subspace of $\\mathsf{V}$ that contains $S$ must also contain the span of $S$</li></ul><p><strong>Definition</strong>: A subset $S$ of a vector space $\\mathsf{V}$ <strong>generates</strong> (or <strong>spans</strong>)$\\mathsf{V}$ if $\\operatorname{span}(S)=\\mathsf{V}$. In this case, we also say that the vectors of $S$ generate (or span)$\\mathsf{V}$.</p><h2 id="_1-5-linear-dependence-and-linear-independence" tabindex="-1">1.5 linear dependence and linear independence <a class="header-anchor" href="#_1-5-linear-dependence-and-linear-independence" aria-label="Permalink to “1.5 linear dependence and linear independence”">​</a></h2><p><strong>Definition</strong>: A subset $S$ of a vector space $\\mathsf{V}$ is called <strong>linearly dependent</strong> if there exists a finite number of distinct vectors $\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_n$ in $S$ and scalars $a_1,a_2,\\cdots ,a_n$, not all zero, such that</p><p>$$a_1\\vec{u}_1+a_2\\vec{u}_2+\\cdots+a_n\\vec{u}_n=\\vec{0}$$</p><p>In this case we also say that the vectors of $S$ are linearly dependent</p><p>For any vectors $\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_n$, We have $a_1\\vec{u}_1+a_2\\vec{u}_2+\\cdots+a_n\\vec{u}_n=\\vec{0}$ if $a_1=a_2=\\cdots =a_n=0$. We call this the <strong>trivial representation</strong> of $\\vec{0}$ as a linear combination of $\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_n$. Thus, for a set to be linearly dependent, there must exists a <strong>nontrivial representation</strong> of $\\vec{0}$ as a linear combination of vectors in the set.</p><p>Consequently, any subset of a vector space that <strong>contains the zero vector</strong> is <strong>linearly dependent</strong> because $\\vec{0} =1\\cdot \\vec{0}$ is a nontrivial representation of 0 as a 1linear combination of vectors in the set.\\</p><p><strong>Definition</strong>: A subset $S$ of a vector space that is not linearly dependent is called <strong>linearly independent</strong>. As before, we also say that the vectors ot $S$ are linearly independent.</p><p>The following facts about linearly independent sets are true in any vector space:</p><ol><li><strong>The empty set is linearly independent</strong>, for linearly dependent sets must be nonempty.</li><li>A set consisting of a single nonzero vector is linearly independent. For if ${\\vec{u}}$ is linearly dependent. then $a\\vec{u}=0$ for some nonzero scalar $a$. Thus</li></ol><p>$$\\vec{u}=a^{-1}(a\\vec{u})=a^{-1}\\vec{0}=\\vec{0}$$</p><ol><li>A set is linearly independent iff the only representations of $\\vec{0}$ as linear combinations of its vectors are <strong>trivial representations</strong></li></ol><h3 id="examples-1" tabindex="-1">Examples <a class="header-anchor" href="#examples-1" aria-label="Permalink to “Examples”">​</a></h3><h4 id="polynominal-2" tabindex="-1">polynominal <a class="header-anchor" href="#polynominal-2" aria-label="Permalink to “polynominal”">​</a></h4><p>For $k=0,1,\\cdots, n$, let $p_k(x)=x^k+x^{k+1}+\\cdots+x^n$, The set $${p_0(x),p_1(x),\\cdots,p_n(x)}$$</p><p>is linearly independent in $\\mathsf{P}_n(F)$.</p><h3 id="theorem" tabindex="-1">theorem <a class="header-anchor" href="#theorem" aria-label="Permalink to “theorem”">​</a></h3><p><strong>Theorem 1.6</strong>: Let $\\mathsf{V}$ be a vector space, and let $S_1 \\subseteq S_2 \\subseteq \\mathsf{V}$.If $S_1$ is</p><p>linearly dependent, then $S_2$ is linearly dependent</p><p><strong>Corollary</strong>: Let $\\mathsf{V}$ be a vector space, $S_1 \\subseteq S_2 \\subseteq \\mathsf{V}$. If $S_2$ is linearly independent, then $S_1$ is linearly independent.</p><p><strong>Theorem 1.7</strong>: Let $S$ be a linearly independent subset of a vector space $\\mathsf{V}$.and let $\\vec{v}$ be a vector in $\\mathsf{V}$ that is not in $S$. Then $S\\cup {\\vec{v}}$ is <strong>linearly dependent</strong> if and only if $\\vec{v} \\in \\operatorname{span}(S)$.</p><h2 id="_1-6-bases-and-dimension" tabindex="-1">1.6 bases and dimension <a class="header-anchor" href="#_1-6-bases-and-dimension" aria-label="Permalink to “1.6 bases and dimension”">​</a></h2><h3 id="basic" tabindex="-1">basic <a class="header-anchor" href="#basic" aria-label="Permalink to “basic”">​</a></h3><p><strong>Definition</strong>: A <strong>basis</strong> $\\beta$ for a vector space $\\mathsf{V}$ is a <strong>linearly independent subset of $\\mathsf{V}$ that generates $\\mathsf{V}$</strong>. If $\\beta$ is a basis for $\\mathsf{V}$, we also say that the vectors of $\\beta$ <strong>form a basis</strong> for V.</p><h4 id="examples-2" tabindex="-1">Examples <a class="header-anchor" href="#examples-2" aria-label="Permalink to “Examples”">​</a></h4><ul><li>Recalling that $\\operatorname{span}(\\phi) = {0}$ and $\\phi$ is linearly independent, we see that is a basis for the <strong>zero vector space</strong>.</li><li>In $\\mathsf{F}^n$, let $\\vec{e}_1 = (1,0,0,\\cdots, 0), \\vec{e}_2=(0,1,0,\\cdots,0),\\cdots,\\vec{e}_n=(0,0,\\cdots,0,1); {\\vec{e}_1,\\vec{e_2},\\cdots,\\vec{e}_n}$ is readily seen to be a basis for $\\mathsf{F}^n$ and is called the <strong>standard basis</strong> for $\\mathsf{F}^n$.</li><li>In $\\mathsf{M}<em>{m\\times n}(F)$, let $E^{ij}$ denote the matrix whose only nonzero entry is a $1$ in the ith row and jth column. Then ${E^{ij}: 1\\le i \\le m, 1 \\le j \\le n}$ is a basis for $\\mathsf{M}</em>{m\\times n}(F)$.</li><li>In $\\mathsf{P}_n (F)$ the set ${1, x, x^2,\\cdots, x^n}$ is a basis. We call this basis the <strong>standard basis</strong> for $\\mathsf{P}_n (F)$.</li></ul><h4 id="theorem-1" tabindex="-1">Theorem <a class="header-anchor" href="#theorem-1" aria-label="Permalink to “Theorem”">​</a></h4><p><strong>Theorem 1.8</strong>: Let $\\mathsf{V}$ be a vector space and $\\beta={\\vec{u}_1, \\vec{u}_2,\\cdot, \\vec{u}_n}$ be a subset of $\\mathsf{V}$. Then $\\beta$ is a basis for $\\mathsf{V}$ if and only if each $\\vec{v}\\in\\mathsf{V}$ can be <strong>uniquely expressed as a linear combination</strong> of vectors of $\\beta$, that is, can be expressed in the form</p><p>$$\\vec{v} = a_1\\vec{u}_1 + a_2\\vec{u}_2 + \\cdots + a_n \\vec{u}_n$$</p><p>for unique scalars $a_1, a_2,\\cdots, a_n$.</p><p><strong>Theorem 1.9</strong>: If a vector space $\\mathsf{V}$ is generated by a finite set $S$, then some subset of $S$ is a basis for $\\mathsf{V}$. Hence $\\mathsf{V}$ has a finite basis</p><p><strong>Theorem 1.10</strong> (Replacement Theorem): Let $\\mathsf{V}$ be a vector space that is generated by a set $G$ containing exactly $n$ vectors, and let $L$ be a linearly independent subset of $\\mathsf{V}$ containing exactly $m$ vectors. Then $m\\le n$ and there exists a subset $H$ of $G$ containing exactly $n – m$ vectors such that $L\\cup H$ generates $\\mathsf{V}$.</p><p><strong>Corollary 1</strong>: Let $\\mathsf{V}$ be a vector space having a finite basis. Then every basis for $\\mathsf{V}$ contains the same number of vectors</p><h3 id="dimension" tabindex="-1">dimension <a class="header-anchor" href="#dimension" aria-label="Permalink to “dimension”">​</a></h3><p><strong>Definitions</strong>: A vector space is called <strong>finite-dimensional</strong> if it has a basis consisting of a finite number of vectors. The unique number of vectors in each basis for $\\mathsf{V}$ is called the dimension of $\\mathsf{V}$ and is denoted by $\\operatorname{dim}(\\mathsf{V})$. A vector space that is not finite-dimensional is called <strong>infinite-dimensional</strong>.</p><h4 id="example" tabindex="-1">Example <a class="header-anchor" href="#example" aria-label="Permalink to “Example”">​</a></h4><ul><li>The vector space ${0}$ has dimension $0$</li><li>The vector space $\\mathsf{F}^n$ has dimension $n$</li><li>The vector space $\\mathsf{M}_{m\\times n}(F)$ has dimension $mn$</li><li>The vector space $\\mathsf{P}_n(F)$ has dimension $n+1$</li></ul><p>The following examples show that the dimension of a vector space depends on its field of scalars.</p><ul><li>Over the field of <em>complex numbers</em>, the vector space of complex numbers has dimension $1$. (A basis is ${1}$.)</li><li>Over the field of <em>real numbers</em>, the vector space of complex numbers has dimension $2$. (A basis is ${1, i}$)</li></ul><h4 id="corollary-and-theorem" tabindex="-1">Corollary and theorem <a class="header-anchor" href="#corollary-and-theorem" aria-label="Permalink to “Corollary and theorem”">​</a></h4><p><strong>Corollary 2</strong>: Let $\\mathsf{V}$ be a vector space with dimension $n$.</p><ol><li>Any finite generating set for $\\mathsf{V}$ contains at least $n$ vectors, and a generating set for $\\mathsf{V}$ that contains exactly $n$ vectors is a basis for $\\mathsf{V}$.</li><li>Any linearly independent subset of $\\mathsf{V}$ that contains exactly is vectors is a basis for $\\mathsf{V}$.</li><li>Every linearly independent subset of $\\mathsf{V}$ can be <strong>extended</strong> to a basis for $\\mathsf{V}$.</li></ol><p><strong>Theorem 1.11</strong>: Let $\\mathsf{W}$ be a subspace of a finite-dimensional vector space $\\mathsf{V}$. Then $\\mathsf{W}$ is finite-dimensional and $\\operatorname{dim}(\\mathsf{W}) &lt; \\operatorname{dim}(\\mathsf{V})$. Moreover, if $\\operatorname{dim}(\\mathsf{W}) = \\operatorname{dim}(\\mathsf{V})$, then $\\mathsf{V} = \\mathsf{W}$.</p><p>The set of diagonal $n\\times n$ matrices is a subspace $\\mathsf{W}$ of $\\mathsf{M}_{n\\times n} (F)$. A basis for $\\mathsf{W}$ is</p><p>$${E^{11}, E^{22},\\cdots,E^{nn}},$$</p><p>where $E^{ij}$ is the matrix in which the only nonzero entry is a 1 in the ith row and jth column. Thus $\\operatorname{dim}(\\mathsf{W})= n$.</p><p><strong>Corollary</strong>: If $\\mathsf{W}$ is a subspace of a finite-dimensional vector space $\\mathsf{V}$, then any basis for $\\mathsf{W}$ can be extended to a basis for $\\mathsf{V}$.</p>',65)]))}const b=n(r,[["render",$]]);export{p as __pageData,b as default};
